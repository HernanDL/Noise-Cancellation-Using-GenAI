{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HernanDL/Noise-Cancellation-Using-GenAI/blob/main/Noise_Cancellation_with_Hugging_Face_Wav2Vec2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YkiEtiMBtdb"
      },
      "source": [
        "# Fine-Tuning Wav2Vec2 for Noise Cancellation (Waveform Cancellation)\n",
        "\n",
        "This notebook fine-tunes the **Wav2Vec2** model from Hugging Face for a specialized task: predicting the **inverse** waveform of an input signal, so that when the input and output signals are combined, the result is silence (destructive interference).\n",
        "\n",
        "## Goal\n",
        "The goal is to train the model to generate a phase-inverted waveform that, when added to the original input signal, produces a silent (flat) waveform."
      ],
      "id": "1YkiEtiMBtdb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKrOP7Q6Btdc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install transformers datasets librosa soundfile torch torchaudio"
      ],
      "id": "cKrOP7Q6Btdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk6ACK3xBtdd"
      },
      "source": [
        "## Step 2: Import Libraries\n",
        "We'll import necessary libraries like Hugging Face's `transformers`, `datasets`, PyTorch, and Librosa for audio processing."
      ],
      "id": "Pk6ACK3xBtdd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOwa85aVBtdd"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "from datasets import Dataset\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ],
      "id": "vOwa85aVBtdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU-nomoBBtde"
      },
      "source": [
        "## Step 3: Load Pre-trained Wav2Vec2 Model\n",
        "We'll load the pre-trained Wav2Vec2 model from Hugging Face's model hub, which will be fine-tuned for the task of waveform inversion (phase shift)."
      ],
      "id": "tU-nomoBBtde"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46P6KBDVBtde"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the Wav2Vec2 model and tokenizer\n",
        "model_name = 'facebook/wav2vec2-base-960h'\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)"
      ],
      "id": "46P6KBDVBtde"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcSKaIVTBtde"
      },
      "source": [
        "## Step 4: Upload Noisy Audio (Input Signal)\n",
        "We will upload a noisy input signal (in `.wav` format), which the model will learn to cancel by generating an inverse waveform."
      ],
      "id": "mcSKaIVTBtde"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic6JQIFjBtde"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Upload your noisy input audio file\n",
        "uploaded = files.upload()\n",
        "input_audio_file = list(uploaded.keys())[0]  # Get uploaded file name\n",
        "\n",
        "# Load the input audio file\n",
        "input_audio, sr = librosa.load(input_audio_file, sr=16000)  # Resample to 16kHz (Wav2Vec2's input requirement)\n",
        "\n",
        "# Plot the input waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(input_audio)\n",
        "plt.title('Input Audio Waveform')\n",
        "plt.show()"
      ],
      "id": "Ic6JQIFjBtde"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIGJv3sZBtde"
      },
      "source": [
        "## Step 5: Generate the Target Inverse Waveform\n",
        "The model will be trained to generate the **inverse** (180-degree phase-shifted) version of the input signal.\n",
        "This will be the target signal for the model's output."
      ],
      "id": "pIGJv3sZBtde"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWAdDhDrBtdf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create the inverse (phase-shifted) waveform\n",
        "inverse_audio = -input_audio  # Simply invert the waveform (180-degree phase shift)\n",
        "\n",
        "# Plot the inverse waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(inverse_audio)\n",
        "plt.title('Inverse (Phase-Shifted) Audio Waveform')\n",
        "plt.show()\n",
        "\n",
        "# Check that adding input_audio and inverse_audio results in silence\n",
        "combined_audio = input_audio + inverse_audio\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(combined_audio)\n",
        "plt.title('Combined Waveform (Input + Inverse)')  # This should be a flat line (silence)\n",
        "plt.show()"
      ],
      "id": "kWAdDhDrBtdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DavDTit1Btdf"
      },
      "source": [
        "## Step 6: Preprocess Data\n",
        "We now preprocess the data by tokenizing the input audio (noisy signal) and setting the inverse audio as the model's target for training."
      ],
      "id": "DavDTit1Btdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6yQx78CBtdf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Tokenize input (noisy) audio\n",
        "input_values = tokenizer(input_audio, return_tensors='pt', padding='longest').input_values\n",
        "\n",
        "# Target is the inverse (phase-shifted) audio\n",
        "labels = torch.tensor([inverse_audio], dtype=torch.float32)\n",
        "\n",
        "# Dataset construction\n",
        "dataset = Dataset.from_dict({\n",
        "    'input_values': input_values.numpy(),\n",
        "    'labels': labels.numpy()\n",
        "})"
      ],
      "id": "q6yQx78CBtdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxfk6AzvBtdf"
      },
      "source": [
        "## Step 7: Fine-Tuning Wav2Vec2 for Waveform Cancellation\n",
        "We define a training loop that fine-tunes the model to predict the inverse waveform."
      ],
      "id": "vxfk6AzvBtdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5IhjAaQBtdf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=2,\n",
        "    evaluation_strategy='steps',\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset  # In practice, you should use a separate validation set\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "id": "y5IhjAaQBtdf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWOMyFKuBtdf"
      },
      "source": [
        "## Step 8: Inference and Visualization\n",
        "After fine-tuning, we'll test the model on the same noisy input and plot the resulting inverse waveform."
      ],
      "id": "cWOMyFKuBtdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU3c_IU8Btdf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Apply the fine-tuned model to generate an inverse waveform\n",
        "input_values = tokenizer(input_audio, return_tensors='pt', padding='longest').input_values\n",
        "with torch.no_grad():\n",
        "    predicted_inverse = model(input_values).logits\n",
        "\n",
        "# Plot the predicted inverse waveform\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(predicted_inverse[0].cpu().numpy())\n",
        "plt.title('Predicted Inverse Waveform')\n",
        "plt.show()\n",
        "\n",
        "# Combine input_audio and predicted_inverse to check cancellation\n",
        "combined_audio = input_audio + predicted_inverse[0].cpu().numpy()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(combined_audio)\n",
        "plt.title('Combined Waveform (Input + Predicted Inverse)')  # Should approach silence\n",
        "plt.show()"
      ],
      "id": "iU3c_IU8Btdf"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}