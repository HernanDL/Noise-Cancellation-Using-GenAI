{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HernanDL/Noise-Cancellation-Using-GenAI/blob/main/Noise_Cancellation_Using_Generative_AI_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-FXsPjGjlKy"
      },
      "source": [
        "# Noise Cancellation Using Generative AI\n",
        "This project implements a noise cancellation system using a Generative Adversarial Network (GAN). It takes noisy audio as input, generates an inverse waveform, and combines the two to cancel the noise, resulting in silence.\n",
        "\n",
        "In this notebook, we'll:\n",
        "1. Load audio and preprocess it.\n",
        "2. Convert it to a spectrogram (frequency-domain representation).\n",
        "3. Build and train a GAN model for predicting inverse waveforms.\n",
        "4. Visualize the results (waveforms and spectrograms).\n",
        "5. Optionally integrate with external APIs (like OpenAI).\n"
      ],
      "id": "M-FXsPjGjlKy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfXS24JkjlK1"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install numpy scipy librosa soundfile matplotlib torch torchaudio tensorflow"
      ],
      "id": "TfXS24JkjlK1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFNYH5A2jlK2"
      },
      "source": [
        "## Step 2: Load and Preprocess Audio Data\n",
        "Let's load the audio file using `librosa` and visualize the waveform.\n",
        "\n",
        "**For Colab:** We will upload the `.wav` file directly to the Colab environment."
      ],
      "id": "cFNYH5A2jlK2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNIIb6fWjlK2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "from google.colab import files\n",
        "\n",
        "# Function to upload files to Colab\n",
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "\n",
        "# Function to load the audio file\n",
        "def load_audio(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    return audio, sr\n",
        "\n",
        "# Function to plot the waveform\n",
        "def plot_waveform(audio, sr):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.waveshow(audio, sr=sr)\n",
        "    plt.title('Waveform')\n",
        "    plt.show()\n",
        "\n",
        "# Load and visualize the audio\n",
        "audio, sr = load_audio(audio_file)\n",
        "plot_waveform(audio, sr)"
      ],
      "id": "VNIIb6fWjlK2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb4RedhSjlK3"
      },
      "source": [
        "## Step 3: Convert Audio to Spectrogram\n",
        "We convert the audio waveform into a spectrogram using the Short-Time Fourier Transform (STFT) for feature extraction."
      ],
      "id": "hb4RedhSjlK3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KozHf4VWjlK3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Convert audio waveform to spectrogram\n",
        "def audio_to_spectrogram(audio, sr):\n",
        "    stft = librosa.stft(audio)\n",
        "    spectrogram = np.abs(stft)\n",
        "    return spectrogram\n",
        "\n",
        "# Plot the spectrogram\n",
        "def plot_spectrogram(spectrogram, sr):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(spectrogram, ref=np.max), sr=sr, x_axis='time', y_axis='log')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Spectrogram')\n",
        "    plt.show()\n",
        "\n",
        "# Example: Convert audio to spectrogram and visualize\n",
        "spectrogram = audio_to_spectrogram(audio, sr)\n",
        "plot_spectrogram(spectrogram, sr)"
      ],
      "id": "KozHf4VWjlK3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnqOFi8UjlK3"
      },
      "source": [
        "## Step 4: Build the Generative Model (GAN)\n",
        "We'll implement the Generator and Discriminator models using PyTorch to predict an inverse waveform."
      ],
      "id": "HnqOFi8UjlK3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTCms-UWjlK3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Generator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # Additional layers can be added here\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Discriminator Model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # Additional layers can be added here\n",
        "            nn.Conv2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ],
      "id": "DTCms-UWjlK3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2gHnh6vjlK3"
      },
      "source": [
        "## Step 5: Define Training Process\n",
        "We'll set up the training loop for both the generator and discriminator. The generator will attempt to produce an inverse waveform, and the discriminator will evaluate how close it is to the real inverse."
      ],
      "id": "L2gHnh6vjlK3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXEcZHq_jlK4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Loss function and optimizers\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy loss for GAN\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "# Training loop\n",
        "def train(model_G, model_D, data_loader, optimizer_G, optimizer_D, criterion, num_epochs=100):\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (noisy, clean) in enumerate(data_loader):\n",
        "            # Optimize the discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "            loss_D = criterion(model_D(...), ...)\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Optimize the generator\n",
        "            optimizer_G.zero_grad()\n",
        "            loss_G = criterion(model_G(...), ...)\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}], Loss_G: {loss_G.item()}, Loss_D: {loss_D.item()}\")\n",
        "\n",
        "# Note: The actual data loader would need to provide batches of noisy and clean pairs of audio.\n",
        "# Replace '...' with appropriate training code for your data loader."
      ],
      "id": "HXEcZHq_jlK4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEL3YM0NjlK4"
      },
      "source": [
        "## Step 6: Inference and Visualization of Results\n",
        "After training, we can generate the inverse waveform and visualize the effect on the original noisy audio."
      ],
      "id": "SEL3YM0NjlK4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL-YJ89ajlK4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Function to reconstruct audio from spectrogram\n",
        "def spectrogram_to_audio(spectrogram, sr):\n",
        "    return librosa.griffinlim(spectrogram)\n",
        "\n",
        "# Example: Generate inverse waveform and combine with original\n",
        "inverse_waveform = spectrogram_to_audio(generator(spectrogram), sr)\n",
        "combined_waveform = audio + inverse_waveform\n",
        "\n",
        "# Plot original and combined waveforms to visualize the cancellation effect\n",
        "plot_waveform(combined_waveform, sr)"
      ],
      "id": "mL-YJ89ajlK4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFUNLo91jlK4"
      },
      "source": [
        "## Step 7: (Optional) Integration with External APIs\n",
        "If you'd prefer to use external APIs such as OpenAI or Google AI, you can integrate them as follows:"
      ],
      "id": "mFUNLo91jlK4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EfUkhwHjlK4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Example: Call OpenAI API (this is hypothetical)\n",
        "def call_openai_audio_model(audio):\n",
        "    response = openai.Audio.create(audio_file=audio, ...)\n",
        "    return response['output_audio']\n",
        "\n",
        "# Example usage\n",
        "output_audio = call_openai_audio_model('path_to_noisy_audio.wav')"
      ],
      "id": "9EfUkhwHjlK4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}